\section{Convolutional side-outputs for image segmentation}
\label{sec:method}

\begin{figure*}[!t]
\centering
\subfloat[Stage Layer Outputs (SLO)]{\input{figures/stage} \label{fig:slo}}
\hfil
\subfloat[All Layers Outputs (ALO)]{\input{figures/full} \label{fig:alo}}
\caption{Illustration for the two side-outputs extraction strategies:  (a) side-outputs extracted at each stage of the network and (b) side-outputs extracted at each convolutional layer}
\label{fig:extraction}
\end{figure*}

Hierarchies are long associated with the image segmentation task~\cite{jones97,cardelino06,najman12,xu16,cousty18}, to a degree that it improves a coherent organization of nested regions. The main motivation for using well-defined hierarchies is that different hierarchical level contains different detail level. In this work, instead of using a well-defined hand-engineered hierarchical structure, it is proposed to explore the concept abstraction resultant of the deep network dynamics, extracting side-outputs at different layers that ideally would contain different level of details. 


The idea is to combine the side-output maps into a single proposition to be evaluated in the image segmentation task, driving the learning flow towards creating adequate regions for the task. In an optimal scenario, the side-outputs would contain enough details to cope with the task, whilst creating coherent region proposals.

Amongst the many strategies for deep models, convolutional networks are well-know by the concept abstraction due the multiple stages of convolution and have been successfully used for the object recognition task. They are usually characterized by three nested functions in multiple layers, namely: (i) convolution; (ii) spatial pooling; and (iii) non-linear activation. Formally, let a convolutional network $\mathit{f}$ composed by $L$ layers be defined as:

\begin{equation}
\mathit{f}(\mathbf{X})=\mathbf{W}_L\mathbf{H}_{L-1}
\end{equation}
\noindent in which:
\begin{itemize}
\item $\mathbf{W}_l$ is the associated weights for the layer $l$;
\item $\mathbf{H}_l$ is the output of the previous layer $l$, defined as
\end{itemize}

\small
\begin{equation}
\mathbf{H}_l= pooling(activation(\mathbf{W}_l\mathbf{H}_{l-1}))~\forall l \in \{1,...,L-1\}
\end{equation}
\normalsize
\noindent For consistency, consider $\mathbf{H}_0=\mathbf{X}=\{X_1, X_2,...,X_n\}$ the set of $N$ input images $I$.

The VGG network~\cite{simonyan2014} is one of the first attempts to create deeper models following the convolutional scheme. The core of the layers in VGG is defined by a convolution immediately followed by a rectified linear unit, as follows:
\begin{equation}
C_l=ReLU(\mathbf{W}_{l}\mathbf{H}_{l-1})~\forall l \in \{1,..,L-1\}
\end{equation}
\noindent in which $ReLU(\cdot)=max(0,\cdot)$. There is also two types of stages, $\mathit{S}^{(1)}$ and $\mathit{S}^{(2)}$, that could formally defined as:
\small
\begin{align}
\mathit{S}^{(1)}&=ReLU(\mathbf{W}_l(ReLU(\mathbf{W}_{l-1}\mathbf{H}_{l-2})))\\
\mathit{S}^{(2)}&=ReLU(\mathbf{W}_l(ReLU(\mathbf{W}_{l-1}(ReLU(\mathbf{W}_{l-2}\mathbf{H}_{l-3})))
\end{align}
\normalsize
\noindent The output of a hidden layer is computed as $maxpool(\mathit{S}^{(1)})$ or $maxpool(\mathit{S}^{(2)})$ for all $S$ stages in the network.

Questions on which and how many side-outputs would be adequate for the image segmentation task, are assessed using two different extraction strategies, both applied in the VGG network. Namely: (i) Stage Layer Outputs~(\textbf{SLO}), inspired by the HED model, creating one side-output for each VGG stage; and (ii) All Layers Outputs~(\textbf{ALO}), inspired by the RCF model, creating one side-output for each convolutional layer. 

Formally, the set $\mathcal{H}$ of $M$ side outputs maps in each strategy is defined as:
\small
\begin{align}
\mathcal{H}_{SLO}=\{\mathcal{H}_1,...,\mathcal{H}_m|& m\in[1,S]~\text{and}~\mathcal{H}_m \in\{S^{(1)},S^{(2)}\}\}\\
\mathcal{H}_{ALO}=\{\mathcal{H}_1,...,\mathcal{H}_m|& m\in[1,L-1]~\text{and}~\nonumber\\&{H}_m=C_l~\forall l \in \{1,..,L-1\}\}
\end{align}
\normalsize

In the case of \textbf{SLO}, the number of side-outputs amounts to the number of pooling layers in the network and for \textbf{ALO}, it is equal to the number of convolutional layers. An illustration for both strategies is presented in Figure~\ref{fig:extraction}.

\begin{comment}
\subsection{Loss evaluation}

For loss evaluation, we used two different functions: the well known categorical cross-entropy and fuse-pixel-error, proposed by \cite{xie2015}.

Formally, categorical cross-entropy for classes $p$ and $q$ and a given set, is defined as:

\begin{equation}
 H(p,q)= {E}_{p}[-\log q]
 \label{eq:single_cross_entropy}
\end{equation}

where $H(p)$ is the entropy of $p$. Once images contains a set of pixels, and each one is defined by Equation  \ref{eq:single_cross_entropy}, the value all pixels set for a single image is defined as:

\begin{equation}
 H(p,q)= -\sum{E}_{p}[\log q]
 \label{eq:cross_entropy}
\end{equation}

Formally, fuse-pixel-error, as defined in \cite{xie2015} consists in the distance $Dist(\cdot , \cdot)$ between the ground-truth label map and the neural net predictions. The loss function $\zeta_{fuse}$ is defined as:

{\color{red}{Nao encontrei o simbolo adequado para formula - utilizei a letra zeta $\zeta$ por enquanto}}
\begin{equation}
 \zeta_{fuse}(W, w, h) = Dist(Y, \hat{Y}_{fuse} )
 \label{eq:fuse_pixel_error}
\end{equation}

, where $\hat{Y}_{fuse} \equiv \sigma(\sum_{m=1}^{M}{h_m \hat{A}^{(m)}_{side}})$ and $\mathbf{h} = (h_1, ..., h_M)$.
\end{comment}

\subsection{Merging strategies}

The main question in dealing with side-outputs in convolutional networks is how to combine them, considering that they are presented in different scales and could represent different concepts. The goal is to produce a single proposition to be evaluated in the task, but retain the useful information presented at different layers.

%\subsection{Side output merging strategies}

In this work, the strategy to overcome those challenges is to combine the side-outputs by exploring the knowledge of the learning process. To achieve that, it is proposed to apply simple merging functions that would enhance different desirable behavior, as described in the following: 
\begin{itemize}
\item \textit{ADD}: Aims to balance negative and positive weights;
\item \textit{AVG}: Aims to create a proposition representing the whole network learning;  
\item \textit{MAX}: Aims to represent confident values. 
\end{itemize}  

Formally, the single proposition $Z$ to be evaluated in the task, under each strategy could be defined as:

\begin{align}
Z_{ADD} &= \sum_{i=1}^{M}(\mathcal{H}_i)\\
Z_{AVG} &= \frac{\sum_{i=1}^{M}(\mathcal{H}_i)}{M}\\
Z_{MAX} &= \max_{1 \leq i \leq M} (\mathcal{H}_i)
\end{align} 

The operations are performed element-wise on each side-output after they are re-scaled for the input size, while maintaining the connectivity pattern. 

Once the combined map is created, it is evaluated on the segmentation task which aims to provide partition of an image into a set of regions representing meaningful areas. This could be reduced to a binary problem aiming to distinguish each pixel of the image as belonging to a region of interest or the background. If confronted with multiple regions of interest this minimal formulation could be executed individually and paired later.

Formally, consider once again the set of $N$ training images $\mathbf{X}$ and alike $\mathbf{Y}=\{Y_1, Y_2,...,Y_n\}$ the set of ground-truth images in which each pixel is labeled.  The ground-truth images are used to calculate the pixel accuracy measuring the rate that a pixel is correctly predicted to belong to the region of interest or the background.

Formally defined in \cite{xie2015}, fuse-pixel-error consists in the distance $Dist(\cdot , \cdot)$ between the ground-truth label map and the neural net predictions. The loss function $\mathcal{L}_{fuse}$ is defined as:

\begin{equation}
 \mathcal{L}_{fuse}(W, w, h) = Dist(Y, \hat{Y}_{fuse} )
 \label{eq:fuse_pixel_error}
\end{equation}

, where $\hat{Y}_{fuse} \equiv \sigma(\sum_{m=1}^{M}{h_m \hat{A}^{(m)}_{side}})$ and $\mathbf{h} = (h_1, ..., h_M)$.

\vspace{0.3cm}

For each merging strategy, each layer learns different information. The merging method influences in how the network leans. It is possible to see how each side output contributes to the final output in Figure \ref{fig:side_outputs}. To simplify the study of side outputs, we decided to visualize only the last  output from each stage in \textbf{ALO} network (that contains 13 side outputs). To make the results more clear, images were also converted to black and white, where white pixels were classified as road and black pixels were classified as background.

\begin{figure*}
  \caption{Side outputs for each merging strategy in ALO network.}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/falreis/side_outputs.png}
  \label{fig:side_outputs}
\end{figure*}

Figure \ref{fig:side_outputs} indicates that the first two side outputs does not produce significant information. Images are almost white, indicating that all pixels were classified as road. ALO-AVG and ALO-ADD third layer contains a clear separation between road pixel than non-road pixels. ALO-MAX's third layer, by other hand, does not clearly separates road from non-road pixels. The results are almost pixelated when compared with the original image.

Figure \ref{fig:side_outputs} also indicates that fourty layer clearly contains the best side output for the evaluated networks. The road marks are clearly visible, but with some noise. ALO-MAX contains a lot of noise, while ALO-ADD contains a few ones. The final side output contains a lot of noise, with results far away worse than the previous layer. This possible indicates that the layer was not able to correctly learn the information from the previous one.

The fuse layer in ALO network with different strategies take all information from side outputs and combine them to produce a single output. Since the network was trained before, merging layers seens to accepted bad results and use them to produce good values.

%. Each one of this images   Let $Z$ be the merged map obtained using one of the proposed functions and $|Z| = |I|, \forall I\in \mathbf{X}$.
%
%$\mathit{S}=\{(\mathit{X_n,Y_n}), \mathit{n}=1,...,\mathit{N}\}$ be the training input set for the network, in which $\mathit{X_n}$ is a set of $\mathit{N}$ images with three color channels and $\mathit{Y_n}$ the set of $\mathit{N}$ labels associated with each image with values belonging to $\{0,1\}$. Consider also $\mathbf{W}$ the layer set of parameters in which
%$\mathbf{w}=\{\mathbf{w}_1,...,\mathbf{w}_M\}$ is the associated weights for each one of the $\mathit{M}$ side output maps. The objective function for training the weights for the $\ell_{side}$ image map could be defined as:
%\begin{equation}
%\mathcal{L}(\mathbf{W},\mathbf{w})=\sum_{m=1}^M\alpha_m\ell_{side}^{(m)}(\mathbf{W},\mathbf{w}_m)
%\end{equation}

\section{Post-processing}


Mathematical morphology is consistent with the non-linear image analysis, presenting solid theoretical foundation and idempotent functions. The formulations are presented in the complete lattice geometric space, in which the functions are performed considering whole sets operating over another whole set. In mathematical morphology, the operators are known a priori and defined using the sets of structuring elements.

In this work, it is proposed to use mathematical morphology as post-processing step, meaning that this step is not inserted in the learning stage. The main goal is to better cope with the fundamental properties of a well-segmented image, particularly, region uniformity and continuity. To achieve that, it is proposed to use a function filter, called area opening, which tend to destroy the small, thin and conspicuous areas.

Formally, let $\hat{Y}\in \mathbb{R}^2$ be the output of a testing image consistent with the representation created by the parameters learned in the network. Consider $B$ a structuring element and $\gamma_B$ the morphological opening produced by it. Consider also $\lambda$ the threshold parameter which will determine how small a certain area must be to be purged. In this case,  $\gamma_B \subseteq \gamma_\lambda$ if and only if $B$ is a finite union of connected components of area greater or equal to $\lambda$. It is expected that this simple additional step could reduce possible noises on the final result, and in this way improve the results.

 












