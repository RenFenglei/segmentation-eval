\section{Introduction}
\label{sec:intro}

Image segmentation refers to the partition of an image into a set of regions representing  meaningful areas, and it is considered as a challenging semantic task, aiming to determine and group uniform regions for analysis. According to~\cite{DOMINGUEZ}, an adequate segmented image should present some fundamental characteristics, such as: (i) region uniformity and homogeneity in  its features, such as gray level, color or texture; (ii) region continuity, without holes; (iii) significant difference between adjacent regions; and (iv) spatial accuracy with smooth boundaries and without raggedness. Image segmentation is still an active topic of research and, usually, it could be divided in two stages~\cite{guigues06}: (i) low-level analysis, which evaluates the pixel characteristics, neighboring relation and it is ideally uncommitted in terms of position, orientation, size and contrast; and (ii) high-level analysis, which maps the low-level characteristics to fulfill the task.  

Recently, deep learning approaches have drastically changed the computational paradigm for visual tasks. The main advantage of deep learning algorithms is that they do not require an engineered model to operate, meaning that they can not only learn the features to represent the data, but also the models to describe them~\cite{goodfellow16}. Facing this new paradigm, hand-crafted features used in the low-level analysis were first replaced by the features learned in deep models~\cite{farabet2013,simonyan2014,lee2015}, which mostly achieved the desirable results. More recently, many proposals explore the learned model for the high-level analysis, in order to create segmentation maps from the outputs of different layers of a deep network~\cite{xie2017,cheng2016,maninis2017,liu2017}. 
These outputs are network samples, which do not influence the architecture and are therefore often called \textit{side-outputs}. 
One challenge on the latter strategy is the combination of the side-outputs from distinct layers, considering that they have different sizes and could represent different aspects of the input.  In this work, we propose some strategies to combine the side-outputs from different layers by using simple merging functions in order to explore useful behavior in the learning process. We also study the amount of combined side-outputs that is needed to create a viable region proposition. Moreover, we propose the use of a post-processing filtering based on mathematical morphology idempotent functions~\cite{najman13} in order to remove some undesirable small segments.


The networks are trained for a road image segmentation task. The goal in this application is to improve the performance of self-driving cars, aiming to distinguish the road from different objects such pedestrians, sidewalks and other vehicles. This road identification can also can be used in road monitoring and traffic management. In this context, providing a robust and reliable segmentation is essential.
 

The remainder of this work is organized as follows. In Section~\ref{sec:related}, related works that characterize the hierarchy of concepts in deep models are described. In Section~\ref{sec:method}, the proposed method and the merging strategies are presented. In Section~\ref{sec:experiments}, a quantitative and qualitative assessment are done. And, finally, in Section~\ref{sec:conclusion}, some conclusions are drawn.
