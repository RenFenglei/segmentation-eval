\section{Experiments}
\label{sec:experiments}

Experiments were conducted in the KITTI Road/Lane dataset, part of KITTI Vision Benchmarking Suite~\cite{KITTI}. The dataset  contains  images for road and lane estimation for the task of image segmentation. It consists of 289 training and 290 test RGB images, with the size of 1242 pixels width and 375 pixels height. \remEwai{\textbf{What is the resolution of the images?}} The ground-truth is manually annotated for two different road types: (i) road, road area composing all lanes; and (ii) lane, lane the vehicle is currently driving on. Some images contains also sidewalks, that was not evaluated in this paper - sidewalks were classified as background. It is important to notice that the ground-truth is only available for training set and the test evaluation should be performed using KITTI Server.

In this work, only the road ground-truths is used and the lane annotations are ignored. This dataset contains the same image with different ground-truths for lane and road estimation. \remEwai{Should be good to show examples of images and groundtruths from the dataset} \remFeli{I think we won't have enought space in this paper}  Then, we prefer to use the road estimation and build the classifier on a binary problem~(road and background). The road type is divided in three different categories of road scenes, namely: (i) uu\_road, urban unmarked; (ii) um\_road, urban marked; and (ii) umm\_road, urban multiple marked lanes.  

To increase the number of images in the training set, a data augmentation procedure is performed. The following transformations were applied: pepper/salt noise, horizontal flipping (mirror), contrast change, brightness change, noise shadow and random rain/snow. Procedures that would create undesired behavior, such as the road in the sky and distortions that would change the nature of the objects in the scene, such as cars and pedestrians were avoided. Augmentation procedures resulted in 2601 images, divided in 2080 samples for training and 521 samples for validation (about 20\%). 


\subsection{Experimental setup}
   
Our networks were build using using Keras \cite{chollet2015keras} with Tensorflow \cite{tensorflow2015-whitepaper}. We used a pre-trained VGG16 model to initialize the weights. Also, we use SGD optimization with learning rate set to 1e-3, decay of 5e-6 and momentum of 0.95. The default batch size contains 16 images. All training experiments were performed in GeForce GTX 1080 8GB GPU.

%For simplicity, in the remaining of this work, the network using the side outputs extracted at each stage of the VGG will be called Stage Layer Outputs~(\textbf{SLO}) and it is composed by $n=5$ side outputs. Similarly, for the side outputs extracted at each convolutional layer, it will be called All Layers Outputs~(\textbf{ALO}) and it is composed by $n=13$ side outputs. For comparison, it is also defined a network similar to VGG, with only the final output, without any side outputs, called No Side Outputs~(\textbf{NSO}).
The \textbf{SLO} network is composed of $n=5$ side outputs, and the \textbf{ALO} network is composed of $n=13$ side outputs. 
The operations to combine side outputs are presented in the name of the methods. The merging operations \textbf{ADD}, \textbf{AVG} and \textbf{MAX} are available for both ALO and SLO methods.
As a baseline, we use the VGG16 network without any side output but only the final output, called No Side Outputs~(\textbf{NSO}).



\subsection{Training results - Methods Comparison}

The first test set was design to identify the best neural network and its best merging methods. We train all nets with all merging methods for 100 epochs to determine which one learns faster and achieves the best results. This conduct lead us to understand how layers can be easily combined to produce outputs with good precision.

Figures \ref{fig:validation_accuracy_pixel_error} presents the relevant curves obtained during the training phase for the proposed approaches. Figure \ref{fig:validation_accuracy_pixel_error}, presents fuse pixel error loss for tested approaches. ALO networks appear to be more stable with a faster decay than NSO and all SLO approaches. Also, it is important to notice that NSO and SLO-MAX produce high instability in the learning course (SLO-MAX seems to overfit around 40 epochs). On the other hand, ALO-AVG presents the best result for the test, followed by ALO-MAX and ALO-ADD merging strategies.

\begin{figure}
  \caption{Pixel Error Validation Loss \remEwai{Only Pixel Error according to the text}}
  \centering
  %\begin{tabular}{ll}
    \includegraphics[width=1.\columnwidth]{figures/falreis/pixel_error.png}
  
    %\includegraphics[width=1.\columnwidth]{figures/falreis/validation_accuracy.png}
  %\end{tabular}
  \label{fig:validation_accuracy_pixel_error}
\end{figure}

From previous graph, it is possible to conclude that ALO networks had superior and more desirable behavior than the SLO and NSO models. It is believed that these results are consequence of the considerably larger amount of side outputs, which create more possibilities of interchangeability between confident values.

\subsection{Side outputs contribution in each merging strategy}
\label{ssec:merging_learn}

For each merging strategy, each layer learns different information. The merging method influences how the network learns. It is possible to see how each side-output contributes to the final output in Figure \ref{fig:side_outputs}. To simplify the study of side outputs, we decided to visualize only the last  output from each stage in \textbf{ALO} network (that contains 13 side outputs). \remEwai{Not clear relatively to Fig. 2} To make the results more clear, images were also converted to black and white, where white pixels were classified as road and black pixels were classified as background.

\begin{figure*}
  \caption{Side outputs for each merging strategy in ALO network.}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/falreis/side_outputs.png}
  \label{fig:side_outputs}
\end{figure*}

\remEwai{Fig. \ref{fig:side_outputs} is not so clear. Notation from the text should be reused: the side output i what is denoted $\mathcal{H}_i$ (if I'm correctly understanding)}

Figure \ref{fig:side_outputs} indicates that the first two side outputs does not produce significant information. Images are almost white, indicating that all pixels were classified as road. ALO-AVG and ALO-ADD third layer contains a clear separation between road pixel than non-road pixels. ALO-MAX's third layer, on the other hand, does not clearly separate road from non-road pixels. The results are almost pixelated when compared with the original image.

Figure \ref{fig:side_outputs} also indicates that fourth layer clearly contains the best side output for the evaluated networks. The road marks are clearly visible, but with some noise. ALO-MAX contains a lot of noise, while ALO-ADD contains a few ones. The final side output contains a lot of noise, with results far away worse than the previous layer. This possibly indicates that the layer was not able to correctly learn the information from the previous one.

The fuse layer in ALO network with different strategies take all information from side outputs and combine them to produce a single output \remEwai{= the proposition $Z$}. Since the network was trained before \remEwai{? Not clear}, merging layers \remEwai{merging layers = fuse layer?} seems to accept bad results and use them to produce good values.

\subsection{Best results}

In order to improve the results, a new set of tests were performed using 500 training epochs. As some networks had a poor performance in the previous test and other tests with different parameters, we decided to evaluate only ALO network in this new round of tests. The categorical cross entropy validation accuracy and pixel error validation loss for ALO nets are available in Figure \ref{fig:val_acc_500_epochs}.

\begin{figure*}
  \caption{Categorical Cross Entropy Validation Accuracy and Pixel Error Validation Loss results for 500 epochs test set}
  \centering
  \begin{tabular}{ll}
    \includegraphics[width=1.\columnwidth]{figures/falreis/val_acc_500_epochs.png}
  
    \includegraphics[width=1.\columnwidth]{figures/falreis/pixel_error_500_epochs.png}
  \end{tabular}%
  \label{fig:val_acc_500_epochs}
\end{figure*}

The best results of both metrics are quite similar for all networks. This indicates absence of a far better method to combine side outputs. The best result for cross entropy validation metric is just \textbf{0.0009} above the worst one (0.0332 for ALO-AVG and 0.0372 for ALO-MAX). For pixel-error validation loss, the best value is just \textbf{0.0040} above the worst one (0.983 for ALO-ADD and 0.9821 for ALO-AVG).

Due to the similarity of the results, we will indicate the best method using the value of validation pixel-error loss metric. For this criteria, ALO-AVG was defined as the best method of our training set.

\subsection{Post-processing using mathematical morphology}

\remEwai{Insert reference to section IV that describes that post-processing. \cite{najman13} should go in section IV.}
After the training procedure, we create a post processing step to reduce possible noises in results proposition. For this, we used the mathematical morphology operation of Opening~\cite{najman13}. This procedure removes small noises created by the foreground~(the road) in the background. We defined a set of kernels with the sizes of $5\times5$, $7\times7$, $9\times9$, $11\times11$ and $13\times13$ and applied them in the images to reduce different sizes of noises. 
\remEwai{I don't understand here. All the structuring elements are applied sequentially? which makes no sense since the opening by the largest structuring element is included in the opening by the smallest structuring element if I'm right. Or are they all tested independently? In this latter case, which size is finally retained?}

A simple comparison is presented in Figure \ref{fig:post_processing_comp}. In this image, we selected an output result that clearly shows the benefits of mathematical morphology post processing. It is possible to see the removal of part of the noise in the far right of the image (\textit{white pixels}). This procedure increases the confidence, as small variations in the results can lead to a potential problem, if used in a self-driving vehicle. 

A side effect of this method is the removal of some points that seems to fit correctly. This situation happens frequently in the base of the road proposition. In Figure \ref{fig:post_processing_comp}, it can be seen in the bottom left and the bottom right of the road (\textit{red pixels}).

\begin{figure}
  \caption{Comparison between ALO-AVG without post processing and ALO-AVG with post-processing with mathematical morphology. In the last picture, \textit{white} pixels represents desirable differences while \textit{red} pixels represents undesirable ones.}
  \centering
  \includegraphics[width=1.\columnwidth]{figures/falreis/post_processing_comparison.png}
  \label{fig:post_processing_comp}
\end{figure}

\subsection{Evaluation results and comparison with the state-of-the-art}

Reminding that the test evaluation could only be performed using KITTI Server, the metrics provided are maximum F1-measure~(MaxF), average precision~(AP), precision~(PRE), recall~(REC), false positive rate~(FPR) and false negative rate~(FNR). 

The server tests were performed using ALO-AVG method, the best one in the training process. To provide succint labels, we will use the name \textbf{ALO-AVG} for the regular approach and \textbf{ALO-AVG-MM} for the version with mathematical morphology post-processing. 

The results achieved  on the test set according to each category in the road scenes are presented in Table~\ref{tab:metrics}. As expected, the ALO-AVG-MM model performs better then the ALO-AVG in almost all the cases. {\color{red}It is also possible to notice that although the post-processing slightly improves the overall performance, it also increases the number of false negatives. This could be an indication that perhaps the applied kernel sizes are not adequate and are removing more of the foreground than desired.}

If compared with the state-of-the-art~(called \textit{PLARD}, an anonymous submission on the KITTI Server platform), the proposed method is comparable and sometimes superior, regarding the maximum F1-measure and the recall metrics. This is due to the fact that although the reported state-of-the-art on the dataset presents a superior average precision, it also almost always presents a higher rate of false positives and negatives. This indicates that the proposed methods are more precise in delineating the regions to be segmented.

\input{tables/kitti-metrics}

A visual representation of the results is presented in Figure \ref{fig:visual_representation}. This image shows the results marked (in green) over the road, to show the performance of our model.

\begin{figure}
  \caption{Visual representation of the results}
  \centering
  \includegraphics[width=1.\columnwidth]{figures/falreis/visual_representation.png}
  \label{fig:visual_representation}
\end{figure}
