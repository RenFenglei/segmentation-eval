\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref} %url
\usepackage{comment}
%\usepackage[caption=false]{subfig}
\input{config/headers}
\input{config/colors}


\ifCLASSOPTIONcompsoc
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfig}
\fi

\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/]
\tikzexternalize[prefix=tikz/,shell escape=-enable-write18]
\tikzset{external/system call= {pdflatex %-save-size=80000 
%                           -pool-size=10000000 
 %                          -extra-mem-top=50000000 
 %                          -extra-mem-bot=10000000 
  %                         -main-memory=90000000 
                           \tikzexternalcheckshellescape 
                           -halt-on-error 
                           -interaction=nonstopmode %-shell-escape
                           -jobname "\image" "\texsource"}} 
                           
%\usepackage{todonotes}
\newcommand{\remSili}[1]{{\color{red}{\tiny [SG]} {\scriptsize #1}}}
\newcommand{\remSimi}[1]{{\color{yellow}{\tiny [SM]} {\scriptsize #1}}}
\newcommand{\remZeni}[1]{{\color{orange}{\tiny [Z]} {\scriptsize #1}}}
\newcommand{\remRaqi}[1]{{\color{magenta}{\tiny [R]} {\scriptsize #1}}}
\newcommand{\remEwai}[1]{{\color{violet}{\tiny [E]} {\scriptsize #1}}}
\newcommand{\remFeli}[1]{{\color{green}{\tiny [F]} {\scriptsize #1\par}}}
                              
\begin{document}

\title{Combining convolutional side-outputs for road image segmentation
\thanks{The authors are grateful to FAPEMIG (PPM 00006-16), CNPq (Universal 421521/2016-3 and PQ 307062/2016-3), CAPES (MAXIMUM STIC-AmSUD 048/14) and PUC Minas for the financial support to this work.}
}

\author{
\IEEEauthorblockN{
Felipe A. L. Reis\textsuperscript{\dag}, Raquel Almeida\textsuperscript{\dag}, Simon Malinowski\textsuperscript{*}, Ewa Kijak\textsuperscript{*} \\ Silvio Jamil F. Guimar\~aes\textsuperscript{\dag} and Zenilton K. G. do Patroc\'inio Jr.\textsuperscript{\dag}
}
\\
\IEEEauthorblockA{
\textsuperscript{\dag}\textit{Audio-Visual Information Processing Laboratory} -- \textit{Pontifical Catholic University of Minas Gerais} \\ 
Belo Horizonte, Minas Gerais, Brazil \\
\{falreis, raquel.almeida.685026\}@sga.pucminas.br, \{sjamil, zenilton\}@pucminas.br
\\ \\
\textsuperscript{*}\textit{Linkmedia} -- Univ Rennes, Inria, CNRS, IRISA\\
Rennes, France \\
\{simon.malinowski, ewa.kijak\}@irisa.fr
}
}

\maketitle

\begin{abstract}
Image segmentation consists in subdivide an image into meaningful areas and objects. It can be use in scene understanding and recognition, in fields like biology, medicine, robotics, satellite imaging, amongs others. Some recent approaches uses convolutional neural networks to achieve this goal. In this work it is proposed to explore the learned model in a deep architecture, study the impact of the amount of side-outputs and evaluate strategies to combine side-outputs extracted at different layers of the network. It is also proposed the use of a post-processing filtering based on mathematical morphology idempotent functions in order to remove some undesirable small segments. Experiments were performed in the public available KITTI Road Dataset for image segmentation. Our comparison shows that the use of multiples side outputs can increase the overall performance of the network, make it easy to train and more stable. Also, for a small number of training epochs (500) we achieved performance just 5\% below to the best algorithm in Kitti Evaluation Server.
\end{abstract}


\begin{IEEEkeywords}
convolutional neural network, image segmentation, mathematical morphology, CNN, region detection, side-outputs, side output, merging strategies
\end{IEEEkeywords}

\input{files/intro}
%\input{files/deep-hier}
\input{files/related}
\input{files/method}
\input{files/experiments}
\input{files/conclusion}

\bibliographystyle{IEEEtran}
\bibliography{2019-ijcnn,dl,hier-seg}

\end{document}
