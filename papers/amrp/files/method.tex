



\section{Hierarchies in deep models}
\label{sec:method}


Deep learning approaches were initially described as black-box methods, meaning that not much were known about the reasoning and decisions of the created models. Much exertion have been applied to investigate the networks operation, whether by methodical experimentation~\cite{ilin17,kuo16,eigen14,zhang17} or visualization methods~\cite{simonyan13,zeiler14}. Those efforts provided more clarity of the hierarchical aspects of the deep features, which allowed researches to explore these aspects in their endeavors. The  hierarchies learned in deep models are categorized as complex concepts build from simpler ones. When applied for object recognition task for instance, the raw pixel on the input layer is learned as segments and parts until the composition of an object concept at the final layer.  

These hierarchies could be particularly observed in convolutional networks, which are a stacked composition of three main layers, namely: (i) convolution; (ii) pooling; and (iii) non-linear activation. In \cite{ilin17} the authors directly assessed the hierarchy of concepts in convolutional networks, analyzing the knowledge representation and the network abstraction at each type of layer.  The authors are capable to demonstrate the generic aspect at earlier stages and the specialization at later layers. The findings are conformed with the expected behavior of convolutional networks, but it is possible to observe that most of the learned abstraction is due the convolutional layers and that the pooling and non-linear layers rarely contribute for increasing  the abstraction level.



\subsection{Merging strategy}

Hierarchies are long associated with the image segmentation task, to a degree that it improves a coherent organization of nested regions. In this work, instead of hand-engineering the hierarchical structures of a typical approach, it is proposed a strategy to merge hierarchical maps created from the outputs at different layers in a convolutional network. 

To merge the side outputs it is taken the result of the most confident side-output by using a $max()$ function on the side output maps. This is equivalent to trust only the most confident value, ignoring low values. This operation does not imply that all network is learning a task, but means that at least one has learned. Formally, the max operation for side outputs can be defined as follows: Let a set of side outputs $\mathcal{H} = \{\mathcal{H}_1,\mathcal{H}_2,..., \mathcal{H}_n\}$. The fused map $m$ could be defined as the Equation \ref{eq:max_operation}

\begin{equation}
m = \max_{1 \leq j \leq n} (\mathcal{H}_j)
\label{eq:max_operation}
\end{equation}

The convolutional network model used in this work is the VGG network~\cite{simonyan2014}, proposed in 2014 as one of the first attempts to create deeper models for the task of object recognition. The architecture is a composition of multiple stacked convolutional layers. Following each two or three layers of convolution is placed a max-pooling layer. Also, all hidden layers are supplied with a ReLU non-linear rectification. Both HED network \cite{xie2017} and, later, RCF \cite{liu2017} were based in VGG for the task of edge detection by removing the final output of the network and create side outputs combined in a new fused output. 

Inspired by the HED model, we created side outputs for each VGG stage, as illustrated in Figure~1a, which amounts to the number of pooling layers on the network. Also, inspired by the RCF model, it is proposed to create one side output for each convolutional layer of the network, as illustrated in Figure~1b. The RCF also adopted a convolution of $1\times1$ in every stage of the network. In this work, otherwise, it is used the side output without any other combination, applying the $max()$ operation with the raw data from each layer.    




