\section{Related work}
\label{sec:related}


In the earlier years of the deep learning resurgence, the proposal in~\cite{farabet2013} tackles the task of scene parsing---segmentation task applied for each pixel of the image, aiming to group pixels composing all the identifiable objects in the scene---using hierarchical trees and deep features alongside. Images are used as input for a convolutional network to extract deep features from multiple scales of the images, and in parallel to construct a segmentation tree representing in its nodes dissimilarities of neighboring pixels. The tree nodes are used to pool the correspondent deep features to be processed by a classifier. The classifier scores are used to create histogram of object classes for each node of the segmentation tree, and the final parsing proposal is built using the class entropy distribution for selecting the nodes that cover the entire image.   This proposal uses hierarchical trees as auxiliary and external structures for the deep model. 

In exploring the hierarchies within the deep model, three main architectures standout in recent years, namely: (i) Holistically-nested Edge Detection~(HED)~\cite{xie2015}; (ii) Convolutional Oriented Boundaries~(COB)~\cite{maninis2017}; and (iii) Rich Convolutional Features~(RCF)~\cite{liu2017}. Those networks explicit explore the hierarchies by extracting side outputs of traditional convolutional networks to create boundary maps. The process to fuse these maps is also inserted in the network, in which it is attributed weights for each map that will be learned individually and determine its contribution on the final evaluation. 

To the best of our knowledge, the first network exploring this strategy was HED~(extended version in~\cite{xie2017}), which applied the boundary maps for the boundary detection task, aiming to identify the limits separating uniform regions. The HED network create an side-output layer at each stage of the VGG16 network~\cite{simonyan2014}, in which the stages are composed by two Convolution+ReLU layers followed by a Max Pooling layer. In HED, each side-output layer is associated with a classifier in a deeply supervised scheme~\cite{lee2015}. The layers create edge maps, which are scaled and fused at the end, to be evaluate by a cost-sensitive function to balance the bias towards not-boundary pixels. The HED network significantly improved the performance in multiple datasets. The extended version also applied the network for the segmentation task. The authors in~\cite{cheng2016} use the edge maps created by the HED network alongside with other features such as brightness, colors, gradient and variance to describe images. The goal of their proposal was to create an efficient framework to be used as real-time segmentation system, focused on a fusion strategy to update region features.


In the COB network, the authors also create edge maps from side activations, differing maily from HED by the attribution to candidate contours the orientation information and weights representing the contour strength. The contour orientations are estimated by approximation to known polygon segments and are used to create segmentations hierarchies. The segments weights are computed based on the candidate contour neighboring region to measure the confidence that the candidate is a boundary line. The weights are thresholded to determine the granularity of the segment when creating the segmentation hierarchy. The network perform well in multiple tasks such object proposal, object detection, semantic contour and segmentation.


Finally, the RCF network applied in the boundary detection task, which differ from HED by three main modifications. The first regards the input layer, in which it is used pyramids to create multiple scales of the images. The scaled images are later interpolated in the output layer, similar to~\cite{farabet2012}. The second modification regards the number of side output maps. RCF creates a side output at each Convolutial+ReLU layer of the VGG16 network, which is believed to create more detailed representations and improve the network accuracy. The last modification is in the loss function and the ground-truth of the datasets. In the ground-truth images, pixels are weighted based on a vote among multiple human-annotated values. Any pixel that not achieve a confidence vote value is disregarded by the loss function in the network. The goal is to reduce inconsistencies in the fallible human annotations and mitigate the network confusion in controversial pixels.  


\begin{figure*}[!ht]
\begin{center}
\begin{tabular}{l}
(1a) Side outputs extracted at each stage\\
\input{figures/stage}\\
(1b) Side outputs extracted at each convolutional layer\\
\input{figures/full}\\
\end{tabular}%
\caption{Illustration for  the proposed side outputs extraction one following the HED model~(a) at each stage of the VGG network and the other following RCF model~(b) at each convolutional layer}
\end{center}
\label{fig:methods}
\end{figure*}